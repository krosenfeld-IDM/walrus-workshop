# Training configuration for SAE model

# Model architecture parameters
model:
  expansion_factor: 8  # Multiplier for latent dimension (latent = d_in * expansion_factor)
  k_active: 32  # Number of active neurons
  k_aux: 2048  # Number of auxiliary neurons (512 in paper)
  dead_window: 5_000_000  # Window for dead neuron tracking

# Training hyperparameters
training:
  split: "train"
  batch_size: 6144 # Use the same size as a single snapshot 6 * 32 * 32
  learning_rate: 0.0003  # 3e-4
  beta: 0.1 # 1/32 in original paper
  epochs: 5
  source_split: "test"
  save_every: "epoch"
  num_workers: 4

# Wandb configuration
wandb:
  use_wandb: true
  wandb_project: "walrus-workshop-feature-collapse"  # Base project name (can be overridden with layer_name)
  wandb_run_name: null  # null will auto-generate a name

# Walrus-specific settings (for train_walrus)
walrus:
  dataset: "shear_flow"
  layer_name: "blocks.20.space_mixing.activation"
  # num_workers: 4
  random_state: 42  # For train/test split
  n_steps_input: 6 # Number of input steps (from data.module_parameters in well_config.yaml)

# 
analysis:
  features:
    top_k: 50 # number of top features to look at
    num_exemplars_per_feature: 20 # number of exemplars to save per feature